{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_we_bert_PLN.ipynb",
      "provenance": [],
      "mount_file_id": "1lvEoWiupJZt2SMm_ZW0yicY23Qub5PvV",
      "authorship_tag": "ABX9TyPcgtqQSYk09HohMMRrUQjz"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnvAU9Dt7qgP"
      },
      "source": [
        "## **Introdução à Word Embeddings**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn3X_ukr73TD"
      },
      "source": [
        "Hipótese distribucional: palavras tem significados parecidos quando são usadas em contextos parecidos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAbSVjjU8CA8"
      },
      "source": [
        "Modelos de linguagem: \n",
        "- predizem a próxima palavra, dado um conjunto de palavras;\n",
        "- usados em processamento de voz, autocorreção de ortografia, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKUkMMlc8wP0"
      },
      "source": [
        "Mas o que é **Word Embeddings**? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGvQcZ0X82KN"
      },
      "source": [
        "É a representação vetorial de uma palavra, atribuir valores aos atributos através de aprendizado de máquinas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnTnMnGQ82nb"
      },
      "source": [
        "- texto ➜ número"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ste9pszm-_Yb"
      },
      "source": [
        "Um dos algoritmos utilizados: *Word2Vec*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUgZQaM9_Hjb"
      },
      "source": [
        "### Utilizando o *word2vec* no *spaCy*:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppB99U-FA5Zz"
      },
      "source": [
        "Encontrar modelos pré treinados e baixar.\n",
        "- [Repositório de Word Embeddings do NILC](http://www.nilc.icmc.usp.br/embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVOspYZF7k9s"
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiOAwGNaB9lI",
        "outputId": "2dc2d54d-095a-4ea5-b7d5-bf80d7a74b3a"
      },
      "source": [
        "!pip install -U spacy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.7/dist-packages (3.0.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: spacy-legacy<3.1.0,>=3.0.4 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.4)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<8.1.0,>=8.0.3 in /usr/local/lib/python3.7/dist-packages (from spacy) (8.0.3)\n",
            "Requirement already satisfied, skipping upgrade: typer<0.4.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.3.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions<4.0.0.0,>=3.7.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from spacy) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: pydantic<1.8.0,>=1.7.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.7.4)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied, skipping upgrade: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.5.2)\n",
            "Requirement already satisfied, skipping upgrade: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.11.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.3->spacy) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: click<7.2.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.4.0,>=0.3.0->spacy) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: smart-open<4.0.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy) (3.0.0)\n",
            "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy) (2.0.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khCl03dRDNQq"
      },
      "source": [
        "Usando as de 100 dimensões como exemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzXrhg6kEzlt"
      },
      "source": [
        "### Modelo **CBOW** como exemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VD-bVNPrB-yM",
        "outputId": "4384b1e3-ca7f-4fe8-ee27-c77cd56fa396"
      },
      "source": [
        "!python -m spacy init vectors pt '/content/drive/MyDrive/Cursos/PLN_USP/embeddings/cbow_s100.zip' vec_spacy_cbow_100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-28 11:06:54.102238: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'pt'\u001b[0m\n",
            "[2021-05-28 11:06:55,917] [INFO] Reading vectors from /content/drive/MyDrive/Cursos/PLN_USP/embeddings/cbow_s100.zip\n",
            "929606it [00:38, 24083.21it/s]\n",
            "[2021-05-28 11:07:34,642] [INFO] Loaded vectors from /content/drive/MyDrive/Cursos/PLN_USP/embeddings/cbow_s100.zip\n",
            "\u001b[38;5;2m✔ Successfully converted 929606 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/vec_spacy_cbow_100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kizGyCoEUW6"
      },
      "source": [
        "from spacy import util as spc_util\n",
        "\n",
        "texto_simples = 'gato cachorro conversar falar'\n",
        "\n",
        "pathw2v = 'vec_spacy_cbow_100'\n",
        "\n",
        "nlpw2v = spc_util.load_model(pathw2v)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHjIwS5gFJ8g",
        "outputId": "48815700-6d03-4536-df43-24847f08a37e"
      },
      "source": [
        "docw2v = nlpw2v(texto_simples)\n",
        "\n",
        "tokens_w2v = [token.orth_ for token in docw2v]\n",
        "\n",
        "tokens_w2v"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gato', 'cachorro', 'conversar', 'falar']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s635QOliFkbK",
        "outputId": "fa4e059b-7ce9-4476-a626-315847222c08"
      },
      "source": [
        "docw2v[0].vector"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.236389, -0.603464, -0.161562, -0.19812 ,  0.227765, -0.139105,\n",
              "        0.166398, -0.296089,  0.128838, -0.240817,  0.195813, -0.047349,\n",
              "       -0.42046 ,  0.21304 ,  0.117412,  0.088539,  0.148387,  0.052073,\n",
              "        0.136064,  0.134528,  0.161985,  0.185564, -0.126994,  0.224053,\n",
              "       -0.267075,  0.052961,  0.055635,  0.076302,  0.095579,  0.240283,\n",
              "       -0.037003, -0.39441 , -0.098924,  0.247828,  0.124387,  0.260114,\n",
              "        0.25561 , -0.27736 ,  0.107813,  0.048234, -0.057004,  0.408355,\n",
              "       -0.093729, -0.122929,  0.072935, -0.08228 ,  0.073258, -0.020372,\n",
              "       -0.392844, -0.227506, -0.018241, -0.322697,  0.234883, -0.259119,\n",
              "       -0.403255,  0.010997,  0.352699,  0.090044, -0.122668, -0.146297,\n",
              "       -0.087796,  0.149924,  0.280711, -0.294955,  0.068481,  0.13756 ,\n",
              "        0.18189 , -0.31254 ,  0.16442 , -0.216146, -0.10583 ,  0.19913 ,\n",
              "       -0.07746 , -0.204225, -0.144298, -0.282948, -0.410612, -0.011469,\n",
              "       -0.475289, -0.290361,  0.116357,  0.198507,  0.342901,  0.086798,\n",
              "        0.370311,  0.143955,  0.061782,  0.09527 , -0.113759, -0.21731 ,\n",
              "        0.043779,  0.104721,  0.089497,  0.370794,  0.174435,  0.099713,\n",
              "       -0.01605 , -0.123966,  0.399982,  0.272378], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozHzatXjFsto",
        "outputId": "872bdfe2-0481-4c9b-95af-e15cc30f854d"
      },
      "source": [
        "len(docw2v[0].vector)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktke6QMNGrYh"
      },
      "source": [
        "gato X cachorro:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVMcEHFVF66M",
        "outputId": "780fcf2c-7911-420c-db59-e9780b761481"
      },
      "source": [
        "docw2v[0].similarity(docw2v[1])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.76577175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84sKNLbRGssl"
      },
      "source": [
        "conversar X falar:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naA0bGT6Gdz5",
        "outputId": "9ee6ad80-3c67-4a56-df5e-66876662d4d9"
      },
      "source": [
        "docw2v[2].similarity(docw2v[3])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7374225"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWCL4D_iIHYd"
      },
      "source": [
        "### Módulo Scikit-Learn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2AKDUdZGjT_",
        "outputId": "1a68de14-2e41-4d85-8eb9-1c799d30904e"
      },
      "source": [
        "!pip install numpy"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8CcbQsUIQTy",
        "outputId": "87f2646f-ca74-46b1-f6f6-e7623283509b"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/eb/a48f25c967526b66d5f1fa7a984594f0bf0a5afafa94a8c4dbc317744620/scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3MB 1.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.24.2 threadpoolctl-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYPJrBPXIS9N"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from spacy import util as spc_util"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJKchOZ0IbKC"
      },
      "source": [
        "pathw2v = 'vec_spacy_cbow_100'\n",
        "\n",
        "nlpw2v = spc_util.load_model(pathw2v)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMzqF5eKIvAx"
      },
      "source": [
        "madri = nlpw2v.vocab['madri']\n",
        "espanha = nlpw2v.vocab['espanha']\n",
        "franca = nlpw2v.vocab['franca']\n",
        "paris = nlpw2v.vocab['paris']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2tZIuUkIyKN",
        "outputId": "20d69283-76a3-4797-869b-78441f8993a4"
      },
      "source": [
        "vector_res = np.array(madri.vector) - np.array(espanha.vector) + np.array(franca.vector)\n",
        "\n",
        "vector_res = vector_res.reshape(1 , -1)\n",
        "\n",
        "vector_res.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qz52L5AyJK3O",
        "outputId": "1fb6b31d-17dd-4d3e-fff1-f384408b8a90"
      },
      "source": [
        "vector_paris = np.array(paris.vector).reshape(1, -1)\n",
        "\n",
        "similarity = cosine_similarity(vector_res, vector_paris)\n",
        "\n",
        "print(similarity)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.21776156]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpRpK5xPKbPB"
      },
      "source": [
        "### **BERT - Buscando o contexto**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6OlMDHOKmiD"
      },
      "source": [
        "Analisa o texto em nível semântico. Objetivo: tentar prever palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sTuzO7eK4CV"
      },
      "source": [
        "Para a ferramenta rodar, é preciso de:\n",
        "- transformadores - arquitetura e bibliotecas;\n",
        "- modelos treinados; \n",
        "- módulo que permita a execução dos modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_wSOmkVwRlj"
      },
      "source": [
        "Modelo em português: [BERTimbau](https://github.com/neuralmind-ai/portuguese-bert)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIaE9vR-Jvj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12b3b933-71d3-4207-bb8e-b5fd93752c00"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\r\u001b[K     |▏                               | 10kB 12.6MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 14.0MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 18.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 14.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 9.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 10.3MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 11.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 11.1MB/s eta 0:00:01\r\u001b[K     |█▎                              | 92kB 11.9MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 12.9MB/s eta 0:00:01\r\u001b[K     |█▋                              | 112kB 12.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 122kB 12.9MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 12.9MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▏                             | 153kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▎                             | 163kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▌                             | 174kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 184kB 12.9MB/s eta 0:00:01\r\u001b[K     |██▊                             | 194kB 12.9MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 12.9MB/s eta 0:00:01\r\u001b[K     |███                             | 215kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▏                            | 225kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 235kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▌                            | 245kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▋                            | 256kB 12.9MB/s eta 0:00:01\r\u001b[K     |███▉                            | 266kB 12.9MB/s eta 0:00:01\r\u001b[K     |████                            | 276kB 12.9MB/s eta 0:00:01\r\u001b[K     |████                            | 286kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▎                           | 296kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 307kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▌                           | 317kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▋                           | 327kB 12.9MB/s eta 0:00:01\r\u001b[K     |████▉                           | 337kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 348kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 358kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 368kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 378kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 389kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 399kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 409kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████                          | 419kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 430kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 440kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 450kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 460kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 471kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 481kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 491kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 501kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 512kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 522kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 532kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 542kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 552kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████                        | 563kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 573kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 583kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 593kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 604kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 614kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 624kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████                       | 634kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 645kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 655kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 665kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 675kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 686kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 696kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 706kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 716kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 727kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 737kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 747kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 757kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 768kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████                     | 778kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 788kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 798kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 808kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 819kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 829kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 839kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 849kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 860kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 870kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 880kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 890kB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 901kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 911kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 921kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 931kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 942kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 952kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 962kB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 972kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 983kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 993kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.6MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.7MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.8MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.9MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.0MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.1MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.2MB 12.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 28.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 43.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, sacremoses, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9_wl65X1G5A"
      },
      "source": [
        "Passo a passo: \n",
        "1. importação do módulo para a execução do modelo: PyToreh;\n",
        "2. implementar os modelos pré treinados: BERTimbau;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urLFHKLI1dMf"
      },
      "source": [
        "Detalhes na variável **text**:\n",
        "- CLS → início da sentença;\n",
        "- SEP → final da sentença;\n",
        "- MASK → parte da sentença que queremos predizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKb8B9WD8jTh"
      },
      "source": [
        "*obs: usando o corpus_text.txt como base:*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtK3ANHNyaKS"
      },
      "source": [
        "### Atividade 1: predizer o complemento de uma sentença"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPsWqWhuwKvH"
      },
      "source": [
        "def calc_score_tarefa1(text, target_word, tokenizer, model, debug = False):\n",
        "\n",
        "  # tokenizar o texto\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  # pegar os índices do texto no vocabulário próprio\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  # pegar os índices das palavras\n",
        "  target_index = tokenizer.convert_tokens_to_ids([target_word])[0]\n",
        "  masked_index = tokenized_text.index('[MASK]') \n",
        "\n",
        "  # gerar um tensor\n",
        "  segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "  # converter índices em tensores\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "  \n",
        "  # predizer tokens\n",
        "  with torch.no_grad():\n",
        "      predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "  # pegar o token de novo só pra conferir a confiança\n",
        "  expected_token = tokenizer.convert_ids_to_tokens([target_index])[0]\n",
        "\n",
        "  # deixar os valores entre 0 e 1\n",
        "  predictions_candidates = torch.sigmoid(predictions[0][0][masked_index])\n",
        "  \n",
        "  predicted_index = torch.argmax(predictions_candidates).item()\n",
        "\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "  # se a palavra não existir no vocabulário\n",
        "  if predicted_token == '[UNK]':\n",
        "    return 0\n",
        "\n",
        "  predictions_candidates = predictions_candidates.cpu().numpy()\n",
        "  target_bert_confiance = predictions_candidates[target_index]\n",
        "  predicted_bert_confience = predictions_candidates[predicted_index]\n",
        "\n",
        "  # se a palavra existir no vocabulário\n",
        "  score = 1 - (target_bert_confiance-predicted_bert_confience if  target_bert_confiance > predicted_bert_confience else predicted_bert_confience-target_bert_confiance)\n",
        "  if debug:\n",
        "    print(\"predicted token ---> \", predicted_token, predicted_bert_confience)\n",
        "    print(\"expected token  ---> \",expected_token , target_bert_confiance)\n",
        "    print(\"Score:\", score)\n",
        "\n",
        "  return score"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mD7ZxaDy2a8W",
        "outputId": "e20104f5-906c-4057-c21f-5e7545827618"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "# carregando o modelo pré treinado\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model.eval()\n",
        "\n",
        "text = '[CLS] O jogo continuou amarrado no terceiro quarto, com as [MASK] levando a melhor sobre os ataques [SEP]'\n",
        "# [MASK] = defesas\n",
        "\n",
        "target_word = 'amparo'\n",
        "\n",
        "# calculando score\n",
        "score = calc_score_tarefa1(text, target_word, tokenizer, model, debug = True)\n",
        "print(\"Score: \", score)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predicted token --->  defesas 0.9999999\n",
            "expected token  --->  [UNK] 0.99746823\n",
            "Score: 0.9974683523178101\n",
            "Score:  0.9974683523178101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJpAzhDn6wk5"
      },
      "source": [
        "### Atividade 2: verificar a similaridade contextual entre duas palavras (predita e esperada) em uma sentença."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQjF3Rnc2flS"
      },
      "source": [
        "def calc_score_tarefa2(text, target_word, predicted_word, tokenizer, model, debug=False):\n",
        "\n",
        "  # tokenizar o texto\n",
        "  tokenized_text = tokenizer.tokenize(text)\n",
        "  # pegar os índices do texto no vocabulário próprio\n",
        "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "  # pegar os índices\n",
        "  target_index = tokenizer.convert_tokens_to_ids([target_word])[0]\n",
        "  predicted_index = tokenizer.convert_tokens_to_ids([predicted_word])[0]\n",
        "  masked_index = tokenized_text.index('[MASK]') \n",
        "\n",
        "  # gerar um tensor\n",
        "  segments_ids = [0] * len(tokenized_text)\n",
        "\n",
        "  # converter índices em tensores\n",
        "  tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  segments_tensors = torch.tensor([segments_ids])\n",
        "\n",
        "  # pegar as palavras\n",
        "  expected_token = tokenizer.convert_ids_to_tokens([target_index])[0]\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "  # predizer os tokens\n",
        "  with torch.no_grad():\n",
        "      predictions = model(tokens_tensor, segments_tensors)\n",
        "\n",
        "  # definindo os valores entre 0 e 1\n",
        "  predictions_candidates = torch.sigmoid(predictions[0][0][masked_index]).cpu().numpy()\n",
        "\n",
        "  # pegar as palavras de novo\n",
        "  expected_token = tokenizer.convert_ids_to_tokens([target_index])[0]\n",
        "  predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
        "\n",
        "  # se a palavra não existir no vocabulário\n",
        "  if predicted_token == '[UNK]':\n",
        "    return 0\n",
        "\n",
        "  target_bert_confiance = predictions_candidates[target_index]\n",
        "  predicted_bert_confience = predictions_candidates[predicted_index]\n",
        "\n",
        "  # se a palavra existir no vocabulário\n",
        "  score = 1 - (target_bert_confiance-predicted_bert_confience if  target_bert_confiance > predicted_bert_confience else predicted_bert_confience-target_bert_confiance)\n",
        "  if debug:\n",
        "    print(\"predicted token ---> \", predicted_token, predicted_bert_confience)\n",
        "    print(\"expected token  ---> \",expected_token , target_bert_confiance)\n",
        "    print(\"Score:\", score)\n",
        "\n",
        "  return score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SqO7dFQ64Jv",
        "outputId": "5ceabf04-f460-44a7-c5c6-4c5f277528c0"
      },
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, BertForMaskedLM\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# carregando modelo pré treinado\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "model = BertForMaskedLM.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model.eval()\n",
        "\n",
        "text = '[CLS] O jogo continuou amarrado no terceiro quarto, com as [MASK] levando a melhor sobre os ataques [SEP]'\n",
        "# [MASK] = defesas\n",
        "\n",
        "# nesse código, nós vamos dar as duas palavras\n",
        "predicted_word = 'defesas'\n",
        "target_word = 'proteção'\n",
        "\n",
        "# calculando score\n",
        "score = calc_score_tarefa2(text, target_word, predicted_word, tokenizer, model, debug=True)\n",
        "print(\"Score: \", score)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "predicted token --->  defesas 0.9999999\n",
            "expected token  --->  proteção 0.9578208\n",
            "Score: 0.9578208923339844\n",
            "Score:  0.9578208923339844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSZn7Dg-641E"
      },
      "source": [
        "*A aula ocorreu no dia 26 de maio de 2021, das 14h às 18h.*"
      ]
    }
  ]
}